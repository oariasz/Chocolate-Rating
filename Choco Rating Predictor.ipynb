{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"estratek_logo.jpg\" style=\"width:543px;height:190px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"flavor_bean.gif\" style=\"width:474px;height:316px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chocolate Rating Predictor\n",
    "\n",
    "This project compares different optimized machine learning models (supervised learning) to predict chocolate rating based in several factors\n",
    "#### Acknowledgements\n",
    "These ratings were compiled by Brady Brelinski, Founding Member of the Manhattan Chocolate Society. For up-to-date information, as well as additional content (including interviews with craft chocolate makers), please see his website: Flavors of Cacao\n",
    "\n",
    "#### Inspiration\n",
    "Where are the best cocoa beans grown?\n",
    "Which countries produce the highest-rated bars?\n",
    "What’s the relationship between cocoa solids percentage and rating?\n",
    "\n",
    "\n",
    "\n",
    "####  Data Source:\n",
    "    - URL: https://flavorsofcacao.com/chocolate_database.html\n",
    "\n",
    "    - Number of Instances: 1795.\n",
    "    - Number of Attributes: 11 + output attribute\n",
    "    - Attribute information:\n",
    "        - Input variables:\n",
    "             1 - Company (Maker-if known): Name of the company manufacturing the bar.\n",
    "             2 - Specific Bean Originor Bar Name: The specific geo-region of origin for the bar.\n",
    "             3 - REF: A value linked to when the review was entered in the database. Higher = more recent.\n",
    "             4 - ReviewDate: Date of publication of the review.\n",
    "             5 - CocoaPercent: Cocoa percentage (darkness) of the chocolate bar being reviewed.\n",
    "             6 - CompanyLocation: Manufacturer base country.\n",
    "        - Output variable:\n",
    "             7 - Rating: Expert rating for the bar.\n",
    "\n",
    "\n",
    "#### ML Models compared:\n",
    "    - XGBoost\n",
    "    - Deep Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Get the Data\n",
    "\n",
    "##### About this dataset\n",
    "\n",
    "Chocolate is one of the most popular candies in the world. Each year, residents of the United States collectively eat more than 2.8 billions pounds. However, not all chocolate bars are created equal! This dataset contains expert ratings of over 1,700 individual chocolate bars, along with information on their regional origin, percentage of cocoa, the variety of chocolate bean used and where the beans were grown.\n",
    "\n",
    "**Flavors of Cacao Rating System:**\n",
    "\n",
    "    5= Elite (Transcending beyond the ordinary limits)\n",
    "    4= Premium (Superior flavor development, character and style)\n",
    "    3= Satisfactory(3.0) to praiseworthy(3.75) (well made with special qualities)\n",
    "    2= Disappointing (Passable but contains at least one significant flaw)\n",
    "    1= Unpleasant (mostly unpalatable)\n",
    "Each chocolate is evaluated from a combination of both objective qualities and subjective interpretation. A rating here only represents an experience with one bar from one batch. Batch numbers, vintages and review dates are included in the database when known.\n",
    "\n",
    "The database is narrowly focused on plain dark chocolate with an aim of appreciating the flavors of the cacao when made into chocolate. The ratings do not reflect health benefits, social missions, or organic status.\n",
    "\n",
    "Flavor is the most important component of the Flavors of Cacao ratings. Diversity, balance, intensity and purity of flavors are all considered. It is possible for a straight forward single note chocolate to rate as high as a complex flavor profile that changes throughout. Genetics, terroir, post harvest techniques, processing and storage can all be discussed when considering the flavor component.\n",
    "\n",
    "Texture has a great impact on the overall experience and it is also possible for texture related issues to impact flavor. It is a good way to evaluate the makers vision, attention to detail and level of proficiency.\n",
    "\n",
    "Aftermelt is the experience after the chocolate has melted. Higher quality chocolate will linger and be long lasting and enjoyable. Since the aftermelt is the last impression you get from the chocolate, it receives equal importance in the overall rating.\n",
    "\n",
    "Overall Opinion is really where the ratings reflect a subjective opinion. Ideally it is my evaluation of whether or not the components above worked together and an opinion on the flavor development, character and style. It is also here where each chocolate can usually be summarized by the most prominent impressions that you would remember about each chocolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file specifying the column names\n",
    "chocolates = pd.read_csv('flavors_of_cacao_orig.csv', names=['company', 'origin', 'ref', 'review', 'cocoa_percent', 'company_location', 'rating', 'bean_type', 'bean_origin'])\n",
    "\n",
    "chocobak = chocolates\n",
    "chocolates.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Preprocessing and Exploratory Analysis\n",
    "\n",
    "1. Check for null values\n",
    "2. Encode categorical features\n",
    "3. Check and handle duplicates\n",
    "4. Remove or impute null values\n",
    "4. Scale features (Standard Scaling)\n",
    "6. Balance the data per rating?  (does it make sense using linear prediction?)\n",
    "7. Describe the data\n",
    "8. Visualize the data\n",
    "9. Data Analysis Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chocolates.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Check and handle duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chocolates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = chocolates[chocolates.duplicated()]\n",
    "print(duplicates.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chocolates.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chocolates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Encode categorical features\n",
    "\n",
    "- The only categorical feature is 'type', with only two possible values: 'white' and 'red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chocolates.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chocolates.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's convert all the numeric columns to numeric formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'cocoa_percent' column to numeric format\n",
    "chocolates['cocoa_percent'] = chocolates['cocoa_percent'].str.rstrip('%').astype('float') / 100\n",
    "\n",
    "# Convert 'review' column to numeric format\n",
    "chocolates['review'] = chocolates['review'].astype('int')\n",
    "\n",
    "# Convert 'ref' column to numeric format since it means some order (higher = more recent)\n",
    "chocolates['ref'] = chocolates['ref'].astype('int')\n",
    "\n",
    "# Convert 'rating' column to numeric format. It could have been a classification using discreet value in 0.25 steps, \n",
    "# but this tiem we will prefer to use a linear regression model to predict a number\n",
    "chocolates['rating'] = chocolates['rating'].astype('float')\n",
    "\n",
    "chocolates.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the categorical columns\n",
    "cat_features = chocolates.select_dtypes(include=['object']).columns.to_list()\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Sparse Matrices (Disabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code shows how to use csr_matrix, a way to compress sparse matrices and still index them as usual.\n",
    "'''\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Create a larger dense matrix\n",
    "dense_matrix = np.array([\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 0, 2, 0, 0],\n",
    "    [0, 3, 0, 0, 0],\n",
    "    [0, 0, 0, 4, 0],\n",
    "    [0, 0, 0, 0, 5]\n",
    "])\n",
    "\n",
    "# Display the dense matrix\n",
    "print(\"Dense Matrix:\")\n",
    "print(dense_matrix)\n",
    "\n",
    "# Convert the dense matrix to a sparse matrix (Compressed Sparse Row format - CSR)\n",
    "sparse_matrix = csr_matrix(dense_matrix)\n",
    "\n",
    "# Display the sparse matrix\n",
    "print(\"\\nSparse Matrix:\")\n",
    "print(sparse_matrix)\n",
    "\n",
    "\n",
    "# Accessing non-zero elements of the sparse matrix\n",
    "print(\"\\nNon-zero elements of the sparse matrix:\")\n",
    "print(sparse_matrix.data)\n",
    "\n",
    "# Calculate sparsity index for dense matrix\n",
    "total_elements_dense = dense_matrix.size\n",
    "non_zero_elements_dense = np.count_nonzero(dense_matrix)\n",
    "sparsity_index_dense = non_zero_elements_dense / total_elements_dense\n",
    "\n",
    "# Calculate sparsity index for sparse matrix\n",
    "total_elements_sparse = sparse_matrix.size\n",
    "non_zero_elements_sparse = sparse_matrix.nnz\n",
    "sparsity_index_sparse = non_zero_elements_sparse / total_elements_sparse\n",
    "\n",
    "# Display sparsity indices\n",
    "print(\"Sparsity Index for Dense Matrix: {:.4f}\".format(sparsity_index_dense))\n",
    "print(\"Sparsity Index for Sparse Matrix: {:.4f}\".format(sparsity_index_sparse))\n",
    "\n",
    "# Example indexing for both dense and sparse matrices\n",
    "row_index = 2\n",
    "column_index = 1\n",
    "\n",
    "# Indexing the dense matrix\n",
    "print(\"\\nValue in dense matrix at ({}, {}): {}\".format(row_index, column_index, dense_matrix[row_index, column_index]))\n",
    "\n",
    "# Indexing the sparse matrix\n",
    "print(\"Value in sparse matrix at ({}, {}): {}\".format(row_index, column_index, sparse_matrix[row_index, column_index]))\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************ CAMBIAR ESTO UNA VEZ SE VAYAN A HOT-ENCODE los features categóricos\n",
    "# encoded_chocolates = pd.get_dummies(chocolates,columns=cat_features,drop_first=True)\n",
    "encoded_chocolates = chocolates    # TEMPORAL\n",
    "encoded_chocolates.info()\n",
    "\n",
    "\n",
    "\n",
    "# sparse_enconded_chocolates = csr_matrix(encoded_chocolates)\n",
    "\n",
    "# print(sparse_enconded_chocolates.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Remove or impute null values\n",
    "- First we check for NaN or null values.\n",
    "- We will use SimpleImputer estimator to impute all the mising values at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace blank values with NaN\n",
    "chocolates.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "encoded_chocolates.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the rows with null values\n",
    "chocolates[chocolates.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Count the null values\n",
    "null_values = chocolates.isna().sum().sum()\n",
    "null_values, len(chocolates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing null categorical values on columns bean_type and bean_origin\n",
    "encoded_chocolates['bean_type'].fillna('Unknown', inplace=True)\n",
    "encoded_chocolates['bean_origin'].fillna('Unknown', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check the null values count again\n",
    "null_values = chocolates.isna().sum().sum()\n",
    "null_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_chocolates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* COMMENT: No hay más valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it is the imputer block code, just in case it was needed. Not this time.\n",
    "'''\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "wines_imputed_matrix = imputer.fit_transform(encoded_wines)\n",
    "imputed_wines = pd.DataFrame(wines_imputed_matrix, columns=encoded_wines.columns)\n",
    "imputed_wines\n",
    "'''\n",
    "\n",
    "imputed_chocolates = encoded_chocolates\n",
    "imputed_chocolates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- HASTA AQUI ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Scale features (Stadard Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "cat_feat_cols = ['company', 'origin', 'company_location','bean_type', 'bean_origin','rating']\n",
    "scaled_features = scaler.fit_transform(imputed_chocolates.drop(cat_feat_cols, axis=1))\n",
    "\n",
    "scaled_chocolate_columns = imputed_chocolates.columns.drop(cat_feat_cols)\n",
    "scaled_chocolates = pd.DataFrame(scaled_features, columns=scaled_chocolate_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Describe the Data\n",
    "1. wines\n",
    "2. encoded_wines\n",
    "3. imputed_wines\n",
    "4. scaled_wines (normalized)\n",
    "5. X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chocolates.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_chocolates.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad vinos por nivel de calidad\n",
    "\n",
    "print(chocolates.rating.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de vinos por tipo\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(chocolates.bean_origin.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5. VIF (Variance Inflation Factor)\n",
    "\n",
    "\n",
    "\n",
    "VIF determnes the stength of  the correlation between the independent variables\n",
    "VIF score of an independent variable represents how well the variable is explained by other independent variables\n",
    "\n",
    "- VIF starts at 1 and has no upper limit\n",
    "- VIF = 1, no correlation between the independent variable and the other variables\n",
    "- VIF  exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others\n",
    "\n",
    "* Removing columns with high VIF scores can help to reduce multicollinearity and improve the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['vif'] = [variance_inflation_factor(scaled_features, i) for i in range(scaled_features.shape[1])]\n",
    "vif['Feaatures'] = scaled_wines_columns\n",
    "\n",
    "# Checking the values...\n",
    "vif\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No columns have a vif > 5, so we leave it as it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.6 Apply Over Sampling Technique\n",
    "\n",
    "This technique is used to modify the unequal data classes to create balanced datasets.\n",
    "When the quantity of data is insufficient, the oversampling method tries to balance by incrementing the size of rare samples.\n",
    "\n",
    "Over sampling techniques for classification problems\n",
    "    1. Random Oversampling\n",
    "    2. Synthetic Minority Oversampling Technique (SMOTE)\n",
    "    3. Adaptive Synthetic Sampling (ADASYN)\n",
    "\n",
    "We will use SMOTE here.\n",
    "\n",
    "##### SMOTE (Synthetic Minority Oversampling Technique)\n",
    "\n",
    "SMOTE works by utilizin K-nearest neigbors algorithm to create synthetic data.\n",
    "In this technique, the minority class is over-sampled by producing synthetic examples rather than by over-sampling with replacement and for each minority class observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE(k_neighbors=4)\n",
    "# transform the dataset\n",
    "\n",
    "X = scaled_wines\n",
    "y = imputed_wines['quality']\n",
    "\n",
    "X_os, y_os = oversample.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_os.shape, y_os.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_os.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the data has been balanced\n",
    "\n",
    "sns.countplot(x=y_os)\n",
    "sns.countplot(x=y, fill=False)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.6 Apply Under Sampling Technique (*** NOT CHOSEN ***)\n",
    "\n",
    "Unlike oversampling, this technique balances the imbalanced dataset by reducing the size of the class. It is tipically used when there is a lot of data (big datasets).\n",
    "\n",
    "Undersampling techniques for classification problems\n",
    "    1. Random undersampling\n",
    "    2. Near Miss Undersampling\n",
    "    3. Tomek Link Undersampling\n",
    "\n",
    "The possible advantage is the reduction in run-time by decreasing the amount of traiining dataset, and also that it helps to solve memory problems.\n",
    "    \n",
    "We will use Near Miss here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "nmiss = NearMiss()\n",
    "\n",
    "# transform the dataset\n",
    "\n",
    "X = scaled_wines\n",
    "y = imputed_wines['quality']\n",
    "\n",
    "X_us, y_us = nmiss.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_us.shape, y_us.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_os.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe the data has been balanced\n",
    "\n",
    "sns.countplot(x=y_us)\n",
    "# sns.countplot(x=y, fill=False)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection will be to use Oversampling.  Too few examples for the undersampled dataset\n",
    "\n",
    "X = X_os\n",
    "y = y_os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(wines['type'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=wines, x=wines['quality'], hue='type', palette='PuRd')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = wines.select_dtypes(include=['number'])\n",
    "corr_matrix = numerical_columns.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(wines, hue='type', palette='PuRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "sns.heatmap(corr_matrix, cmap='PuOr', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Red vs. White Wine Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_corr = red_wines.corr()\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.heatmap(red_corr, cmap='PuOr', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The most negatively correlated variable with quality is Volatile Acidity and Sulfur Dioxide.\n",
    "* The most positively correlated variable with quality is Alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_corr = white_wines.corr()\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.heatmap(red_corr, cmap='PuOr', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Same results as with red wines regarding correlation with quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines.hist(figsize=(10, 10), bins=60, color='darkred')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the filtered DataFrames to NumPy arrays before passing to histplot\n",
    "red_alcohol = np.array(wines[wines['type'] == 'red']['alcohol'])\n",
    "red_alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the filtered DataFrames to NumPy arrays before passing to histplot\n",
    "white_alcohol = np.array(wines[wines['type'] == 'white']['alcohol'])\n",
    "\n",
    "white_alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the histograms\n",
    "# sns.histplot(data=red_alcohol, alpha=0.4, bins='auto', kde=True, color='red')\n",
    "# sns.histplot(data=white_alcohol, alpha=0.4, bins='auto', kde=True, color='gray')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the lmplot\n",
    "sns.lmplot(\n",
    "    x='free sulfur dioxide',\n",
    "    y='total sulfur dioxide',\n",
    "    # x='alcohol',\n",
    "    # y='residual sugar',\n",
    "    data=wines,\n",
    "    hue='type',\n",
    "    palette={'white': 'gray', 'red': 'darkred'},\n",
    "    height=8\n",
    ")\n",
    "\n",
    "# Use more informative axis labels than are provided by default\n",
    "# sns.set_axis_labels(\"Alcohol level (%)\", \"Residual Sugar\")\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title(\"Alcohol vs. Residual Sugar in Wine\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CONCLUSIONS\n",
    "\n",
    "1. Data is unbalanced with the number of samples per quality values\n",
    "2. WE need to apply data sampling mathods for Imbalanced datasets\n",
    "3. Methods:\n",
    "    Oversampling:\n",
    "    3.1. Random Oversampling\n",
    "    3.2. Synthetic Minority Oversampling Technique (SMOTE)\n",
    "    3.3. Adaptive Synthetic Sampling (ADASYN)\n",
    "    Undersampling:\n",
    "    3.4. Random under sampling\n",
    "    3.5. Near Miss Under Sampling\n",
    "    3.6. Tomek Links Under Sampling\n",
    "4. We corrected data imbalance using oversampling (SMOTE) technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Model training class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class ScoreLogger:\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame(columns=['Model', 'Score'])\n",
    "\n",
    "    def add(self, epic, model, score):\n",
    "        # Get the now date time\n",
    "        now_ts = datetime.datetime.now()\n",
    "        new_row = pd.DataFrame({'Timestamp': [now_ts], 'Epic': [epic], 'Model': [model], 'Score': [score]})\n",
    "        self.df = pd.concat([self.df, new_row], ignore_index=True)\n",
    "\n",
    "    def print(self):\n",
    "        if (len(self.df) == 0):\n",
    "            print('Nothing to show here.')\n",
    "        else:\n",
    "            self.df = self.df.sort_values(by=['Score'], ascending=False)\n",
    "            print(self.df.to_string())\n",
    "            print('\\n')\n",
    "            print('Timestamp:',  self.df['Timestamp'].iloc[0])\n",
    "            print('Best epic:',  self.df['Epic'].iloc[0])\n",
    "            print('Best model:', self.df['Model'].iloc[0])\n",
    "            print('Best score:', self.df['Score'].iloc[0])\n",
    "    def clear(self):\n",
    "        self.df = pd.DataFrame(columns=['Model', 'Score'])\n",
    "\n",
    "logger = ScoreLogger()\n",
    "# logger.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting all the training source data: X, y, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Uncomment in case you DON'T want to use oversampling or undersampling\n",
    "# X = scaled_wines\n",
    "# y = imputed_wines['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.3, random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=200, random_state=101)\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all LogisticRegression hyperparameters\n",
    "\n",
    "# Get the default parameters\n",
    "default_parameters = LogisticRegression().get_params()\n",
    "\n",
    "\n",
    "# Print the default parameters\n",
    "print('Parameter             Value')\n",
    "print('-'*30)\n",
    "for parameter, value in default_parameters.items():\n",
    "    print(f\"{parameter:20}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "hyperparameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['lbfgs', 'sag', 'saga'],\n",
    "    'tol': [0.0001, 0.00001],\n",
    "    'max_iter': [1000, 2000, 5000],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C A U T I O N !!!     7min process ahead\n",
    "\n",
    "# Run the model with all the parameter combinations\n",
    "grid_search = GridSearchCV(log_reg, hyperparameters, cv=5, verbose=3)\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST FOUND LOOKING AT THE GRIDSEARCH RUN\n",
    "# C=10, max_iter=2000, penalty=l2, solver=sag, tol=1e-05;, score=0.500 total time=   1.7s\n",
    "# C=1, max_iter=1000, penalty=l1, solver=saga, tol=0.0001;, score=0.497 total time=   1.3s\n",
    "# C=10, max_iter=5000, penalty=l2, solver=sag, tol=0.0001;, score=0.451 total time=   2.8s\n",
    "# C=10, max_iter=5000, penalty=l2, solver=sag, tol=0.0001;, score=0.497 total time=   2.6s\n",
    "# Get the best hyperparameters\n",
    "# \n",
    "# best_hyperparameters = grid_search.best_params_\n",
    "# print(best_hyperparameters)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "# best_hyperparameters = grid_search.best_params_\n",
    "# print(best_hyperparameters)\n",
    "# print(grid_search.best_score_)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_hyperparameters = {'C': 0.15, 'max_iter': 5000, 'penalty': 'l1', 'solver': 'saga', 'tol': 0.000001}   # Found manually\n",
    "log_reg.set_params(**best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "# accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "# print('Accuracy:', accuracy)\n",
    "\n",
    "print ('Score', log_reg.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression done!')\n",
    "logger.add(this_epic, 'LogisticRegression', log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "# log_reg.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a K Value\n",
    "\n",
    "Let's go ahead and use the elbow method to pick a good K Value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = []\n",
    "\n",
    "# Will take some time\n",
    "for i in range(1,40):\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We choose K = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all LogisticRegression hyperparameters\n",
    "\n",
    "# Get the default parameters\n",
    "default_parameters = KNeighborsClassifier().get_params()\n",
    "\n",
    "\n",
    "# Print the default parameters\n",
    "print('Parameter             Value')\n",
    "print('-'*30)\n",
    "for parameter, value in default_parameters.items():\n",
    "    print(f\"{parameter:20}: {value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "hyperparameters = {\n",
    "    # 'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'n_neighbors': [30],                # Chosen as the best K according to the Elbow chart\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [30, 50, 70, 90, 110],\n",
    "    'p': [1, 2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C A U T I O N !!!     5min process ahead\n",
    "\n",
    "# Run the model with all the parameter combinations\n",
    "grid_search = GridSearchCV(knn, hyperparameters, cv=5, verbose=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model with the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hyperparameters = grid_search.best_params_\n",
    "print(best_hyperparameters)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparameters = grid_search.best_params_\n",
    "print(best_hyperparameters)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "knn.set_params(**best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "# accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "# print('Accuracy:', accuracy)\n",
    "\n",
    "print ('Score', knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('K Nearest Neighbors done!')\n",
    "logger.add(this_epic, 'KNN', knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "# log_reg.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all LogisticRegression hyperparameters\n",
    "\n",
    "# Get the default parameters\n",
    "default_parameters = DecisionTreeClassifier().get_params()\n",
    "\n",
    "\n",
    "# Print the default parameters\n",
    "print('Parameter             Value')\n",
    "print('-'*30)\n",
    "for parameter, value in default_parameters.items():\n",
    "    print(f\"{parameter:20}: {value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "    'min_samples_split': [2, 5, 10, 20, 50],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C A U T I O N !!!     1min process ahead\n",
    "\n",
    "# Run the model with all the parameter combinations\n",
    "grid_search = GridSearchCV(dtree, hyperparameter_grid, cv=5, verbose=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model with the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hyperparameters = grid_search.best_params_\n",
    "print(best_hyperparameters)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparameters = grid_search.best_params_\n",
    "print(best_hyperparameters)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "dtree.set_params(**best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test set\n",
    "y_pred = dtree.predict(X_test)\n",
    "# accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "# print('Accuracy:', accuracy)\n",
    "\n",
    "print ('Score', dtree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree done!')\n",
    "logger.add(this_epic, 'DecisionTree', dtree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rforest = RandomForestClassifier()\n",
    "rforest.fit(X_train, y_train)\n",
    "y_pred = rforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "# log_reg.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all LogisticRegression hyperparameters\n",
    "\n",
    "# Get the default parameters\n",
    "default_parameters = RandomForestClassifier().get_params()\n",
    "\n",
    "\n",
    "# Print the default parameters\n",
    "print('Parameter             Value')\n",
    "print('-'*30)\n",
    "for parameter, value in default_parameters.items():\n",
    "    print(f\"{parameter:20}: {value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'n_estimators': [1000],\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "    'min_samples_split': [2, 5, 10, 20, 50],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C A U T I O N !!!     45 min process ahead\n",
    "\n",
    "# Run the model with all the parameter combinations\n",
    "grid_search = GridSearchCV(rforest, hyperparameter_grid, cv=5, verbose=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model with the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hyperparameters = grid_search.best_params_\n",
    "print(best_hyperparameters)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparameters = grid_search.best_params_\n",
    "print(best_hyperparameters)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_hyperparameters = {'bootstrap': True, 'criterion': 'entropy', 'max_depth': 11, 'min_samples_split': 2, 'n_estimators': 1000}    # REMOVE!!!!\n",
    "rforest.set_params(**best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test set\n",
    "y_pred = rforest.predict(X_test)\n",
    "# accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "# print('Accuracy:', accuracy)\n",
    "\n",
    "print ('Score', rforest.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training set using the RandomForestClassifier object\n",
    "y_pred_train = rforest.predict(X_train)\n",
    "\n",
    "# Calculate the OOB error\n",
    "oob_error = np.mean(y_pred_train != y_train)\n",
    "\n",
    "# Print the OOB error\n",
    "print('OOB error:', oob_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check bias vs. variance calculating the mean error between trainiing set and test set\n",
    "trainset_error = oob_error\n",
    "testset_error = np.mean(y_test != y_pred)\n",
    "print('Train set error', trainset_error)\n",
    "print('Test set error', testset_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forest done!')\n",
    "logger.add(this_epic, 'Random Forest', rforest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "# log_reg.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all SVM hyperparameters\n",
    "\n",
    "# Get the default parameters\n",
    "default_parameters = SVC().get_params()\n",
    "\n",
    "\n",
    "# Print the default parameters\n",
    "print('Parameter             Value')\n",
    "print('-'*30)\n",
    "for parameter, value in default_parameters.items():\n",
    "    print(f\"{parameter:20}: {value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['auto', 'scale'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C A U T I O N !!!     6 min process ahead\n",
    "\n",
    "# Run the model with all the parameter combinations\n",
    "grid_search = GridSearchCV(svm, hyperparameter_grid, cv=5, verbose=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model with the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hyperparameters = grid_search.best_params_\n",
    "print(best_hyperparameters)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hyperparameters = grid_search.best_params_\n",
    "print(best_hyperparameters)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Train the model with the best hyperparameters   (CHANGE THE ESTIMATOR)\n",
    "svm.set_params(**best_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "# accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "# print('Accuracy:', accuracy)\n",
    "\n",
    "print ('Score',svm.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training set using the RandomForestClassifier object\n",
    "y_pred_train = rforest.predict(X_train)\n",
    "\n",
    "# Calculate the OOB error\n",
    "oob_error = np.mean(y_pred_train != y_train)\n",
    "\n",
    "# Print the OOB error\n",
    "print('OOB error:', oob_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check bias vs. variance calculating the mean error between trainiing set and test set\n",
    "trainset_error = oob_error\n",
    "testset_error = np.mean(y_test != y_pred)\n",
    "print('Train set error', trainset_error)\n",
    "print('Test set error', testset_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Support Vector Machine done!')\n",
    "logger.add(this_epic, 'SVM', svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the class labels\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.fit_transform(y_test)\n",
    "\n",
    "xgb_model = XGBClassifier(objective='multiclass:softmax', learning_rate = 0.1,\n",
    "              max_depth = 1, n_estimators = 330)\n",
    "\n",
    "xgb_model.fit(X_train, y_train_encoded)\n",
    "y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all XGBoost hyperparameters\n",
    "\n",
    "# Get the default parameters\n",
    "default_parameters = XGBClassifier().get_params()\n",
    "\n",
    "\n",
    "# Print the default parameters\n",
    "print('Parameter             Value')\n",
    "print('-'*30)\n",
    "for parameter, value in default_parameters.items():\n",
    "    print(f\"{parameter:20}: {value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=hyperparameter_grid, cv=5, verbose=3)\n",
    "random_search.fit(X_train, y_train_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C A U T I O N !!!     6 min process ahead\n",
    "\n",
    "# Run the model with all the parameter combinations\n",
    "grid_search = GridSearchCV(xgb_model, hyperparameter_grid, cv=5, verbose=3)\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model with the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hyperparameters = grid_search.best_params_\n",
    "print(best_hyperparameters)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_random_hyperparameters = random_search.best_params_\n",
    "best_grid_hyperparameters    = grid_search.best_params_\n",
    "print('RandomizedSearch')\n",
    "print(random_search.best_score_)\n",
    "print(best_random_hyperparameters)\n",
    "print('GridSearch')\n",
    "print(best_grid_hyperparameters)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Train the model with the best hyperparameters   (CHANGE THE ESTIMATOR)\n",
    "xgb_model.set_params(**best_grid_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "# accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "# print('Accuracy:', accuracy)\n",
    "\n",
    "print ('Score',xgb_model.score(X_test, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix:') \n",
    "\n",
    "print(confusion_matrix(y_test_encoded, y_pred))\n",
    "sns.heatmap(confusion_matrix(y_test_encoded, y_pred))\n",
    "print('Classification Report:') \n",
    "print(classification_report(y_test_encoded, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training set using the RandomForestClassifier object\n",
    "y_pred_train = xgb_model.predict(X_train)\n",
    "\n",
    "# Calculate the OOB error\n",
    "oob_error = np.mean(y_pred_train != y_train_encoded)\n",
    "\n",
    "# Print the OOB error\n",
    "print('OOB error:', oob_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check bias vs. variance calculating the mean error between trainiing set and test set\n",
    "trainset_error = oob_error\n",
    "testset_error = np.mean(y_test_encoded != y_pred)\n",
    "print('Train set error', trainset_error)\n",
    "print('Test set error', testset_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred_encoded = xgb_model.predict(X_test)\n",
    "\n",
    "# Decode the class labels\n",
    "y_pred = le.inverse_transform(y_pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('XGBoost done!')\n",
    "logger.add(this_epic, 'XGBoost', xgb_model.score(X_test, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Final Tuning for the winner model = XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the class labels\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.fit_transform(y_test)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "\n",
    "\n",
    "xgb_model.fit(X_train, y_train_encoded, eval_set=[(X_test, y_test_encoded)])\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all XGBoost hyperparameters\n",
    "\n",
    "# Get the default parameters\n",
    "default_parameters = XGBClassifier().get_params()\n",
    "\n",
    "\n",
    "# Print the default parameters\n",
    "print('Parameter             Value')\n",
    "print('-'*30)\n",
    "for parameter, value in default_parameters.items():\n",
    "    print(f\"{parameter:20}: {value}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [7],\n",
    "    'min_child_weight': [1, 2],\n",
    "    'early_stopping_rounds': [10, 20],\n",
    "    'learning_rate': [0.3],\n",
    "}\n",
    "\n",
    "# GridSearch\n",
    "# {'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 300}\n",
    "# 0.7797805560613957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C A U T I O N !!!     6 min process ahead\n",
    "\n",
    "# Run the model with all the parameter combinations\n",
    "grid_search = GridSearchCV(xgb_model, hyperparameter_grid, cv=5, verbose=3)\n",
    "grid_search.fit(X_train, y_train_encoded, eval_set=[(X_test, y_test_encoded)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train the model with the best hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the best hyperparameters\n",
    "best_grid_hyperparameters    = grid_search.best_params_\n",
    "\n",
    "print('GridSearch')\n",
    "print(best_grid_hyperparameters)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Train the model with the best hyperparameters   (CHANGE THE ESTIMATOR)\n",
    "xgb_model.set_params(**best_grid_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# reproducibility\n",
    "seed = 101\n",
    "np.random.seed(seed)\n",
    "\n",
    "y_encoded = pd.concat([pd.DataFrame(y_train_encoded), pd.DataFrame(y_test_encoded)])\n",
    "print('y_train_encoded: ', len(y_train_encoded))\n",
    "print('y_test_encoded: ', len(y_train_encoded))\n",
    "print('y_encoded: ', len(y_train_encoded))\n",
    "y_encoded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "default_params = {\n",
    "    # 'objective': 'binary:logistic',\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 2,\n",
    "    'learning_rate': 0.3,\n",
    "}\n",
    "\n",
    "n_estimators_range = np.linspace(1, 200, 10).astype('int')\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    XGBClassifier(**default_params),\n",
    "    X, y_encoded,\n",
    "    param_name = 'n_estimators',\n",
    "    param_range = n_estimators_range,\n",
    "    cv=cv,\n",
    "    scoring='accuracy', \n",
    "    verbose=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the validation curve plot\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6), dpi=100)\n",
    "\n",
    "plt.title(\"Validation Curve with XGBoost (eta = 0.3)\")\n",
    "plt.xlabel(\"number of trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "\n",
    "plt.plot(n_estimators_range,\n",
    "             train_scores_mean,\n",
    "             label=\"Training score\",\n",
    "             color=\"r\")\n",
    "\n",
    "plt.plot(n_estimators_range,\n",
    "             test_scores_mean,\n",
    "             label=\"Cross-validation score\",\n",
    "             color=\"g\")\n",
    "\n",
    "plt.fill_between(n_estimators_range,\n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std,\n",
    "                 alpha=0.2, color=\"r\")\n",
    "\n",
    "plt.fill_between(n_estimators_range,\n",
    "                 test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std,\n",
    "                 alpha=0.2, color=\"g\")\n",
    "\n",
    "plt.axhline(y=1, color='k', ls='dashed')\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "i = np.argmax(test_scores_mean)\n",
    "print(\"Best cross-validation result ({0:.2f}) obtained for {1} trees\".format(test_scores_mean[i], n_estimators_range[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually check if the prediction rates are true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_train_df = pd.DataFrame(y_pred_train, columns=['y_pred_train'])\n",
    "out_train_df = pd.concat([X_train, y_train, y_pred_train_df], ignore_index=True, sort=False, axis=1)\n",
    "out_train_df.columns = X_train.columns.to_list() + ['y_train'] + ['y_pred_train']\n",
    "out_train_df\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=['y_pred'])\n",
    "type(X_test), type(y_test), type(y_pred_df)\n",
    "\n",
    "out_test_df = pd.concat([X_test, pd.Series(y_test), y_pred_df], ignore_index=True, sort=False, axis=1)\n",
    "out_test_df.columns = X_test.columns.to_list() + ['y_test'] + ['y_pred']\n",
    "out_test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=['y_pred'])\n",
    "out_test_df = pd.concat([X_test, pd.Series(y_test), y_pred_df], ignore_index=True, sort=False, axis=1)\n",
    "out_test_df.columns = X_test.columns.to_list() + ['y_test'] + ['y_pred']\n",
    "out_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train_df.to_excel('out_train.xlsx', sheet_name='Training', index=False)\n",
    "out_test_df.to_excel('out_test.xlsx', sheet_name='Test', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "\n",
    "xgb_model.save_model('xgb_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
